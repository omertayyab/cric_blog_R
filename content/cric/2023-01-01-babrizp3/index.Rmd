---
title: The Babar Rizwan Question, Part 3
author: OT
date: '2022-12-15'
slug: BabRizP3
categories:
  - analysis
  - cricket
  - data
  - data science
  - R
tags:
  - analysis
  - cricket
  - data
  - data science
  - plot
  - R Markdown
  - regression
  - correlation
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(here)
library(cricketdata)
library(tidyverse)
library(knitr)
library(kableExtra)
library(gt)
library(ggplot2)

knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE, message = FALSE)
all_data <- readRDS(here("static","data","R_objects","all_data.rds"))
Pak_data <- readRDS(here("static","data","R_objects","Pak_data.rds"))
overs <- readRDS(here("static","data","R_objects","overs.rds"))
result_wk1_pp <- readRDS(here("static","data","R_objects","result_wk1_pp.rds"))
`%!in%` <- Negate(`%in%`)
```

This is an R Markdown document^[Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For downloading the R Markdown file that generated this webpage, visit my [GitHub](https://github.com/omertayyab/cric_blog_R). For more details on using R Markdown, use this [link](http://rmarkdown.rstudio.com)].

# Introduction

In the previous part, we had scene the table below:
```{r Pakistan_matches, echo=FALSE, cache=TRUE}
options(kableExtra.html.bsTable = TRUE)

result_wk1_pp %>%
  select(.,c(1,4:8)) %>%
  kbl(.,
      align = "c",
      digits=1,
      col.names = gsub("_"," ",colnames(result_wk1_pp)[c(1,4:8)])
      ) %>%
  kable_styling(bootstrap_options = c("striped","hover"))
```
We had concluded that the Powerplay impact of the openers was quite close to other top teams so it must actually be the later phases of the innings where Pakistan is lacking (as shown by the difference in final scores). However, we'd still like to quantify how PP scores affect the final score, so we'll look at that question.

We can also see here that Pakistan actually lost more wickets than other top teams at the end of their innings, while losing less in the Powerplay. All of this information points to the direction of Pak's usage of the overs 7-20. We'll look at that question in part 4.

# Powerplay and Final Score

Finally we get to do some data science after all the data wrangling of the last 2 parts! We will start off slowly. It is reasonable to expect that the final total of a team must have some positive correlation with the score in the Powerplay. So, we'll take the 2 elements of the PP score that could have positive correlation with the final total (i.e. runs and wickets remaining) as independent variables and run some type of regression (we'll figure this out below) against the final score (set as the dependent variable).

## Setting up the regression
Before we run a regression,we have to check that the independent variables are not correlated to each other because that violates a key assumption of the linear regression model. We will also have to decide on a sample size. Since we are looking at a general relationship between Powerplay performance and final scores, we don't really have any limitations and we can look at the entire history of T20I games for our top 5 teams plus West Indies, Australia and Sri Lanka (all have been at or near the top even though they might not be there in recent times). Our dataset will have the team names as a column so we can filter them out later if needed and the advantage is that we get a larger sample size which can (hopefully) make our prediction model stronger.

However, we have to limit ourselves to first innings only because during the 2nd innings, the final score has an upper bound (i.e. the target for the chasing team). This could be a big factor in determining the chasing team's approach and we want to avoid that biasing our sample.

# Putting the Dataset together
We have more than 1600 `.csv` files with ball-by-ball data in our collection for all T20Is played since the start of the format. From those, we only really need three things:

1. Powerplay total
2. Powerplay wickets
3. Final Score

So, we'll first filter the `match_id` from our master file which has summary information about each game we have ball-by-ball data for. We'll use the master file to filter out the games containing our chosen 8 teams where they played against each other. This will ensure that the quality of teams remains consistent.

Once we have the `match_id` column that we need, we'll use it to call each `.csv` as they are named according to their `match_id`. We'll read each file and pull out the required three bits of information. When we have the three bits, we'll split them into two tables, one for the end of the Powerplay and one for the final ball of the innings.


```{r data_full, cache=TRUE}
corr_games <-
  all_data %>%
  arrange(.,date) %>%
  filter(outcome != "no result" | is.na(outcome) ) %>%
  filter(date >= lubridate::ymd("1999-01-01") & date <= lubridate::ymd("2022-10-16")) %>%
  filter( team1 %in% c("Pakistan","England","India","New Zealand", "South Africa", "Australia", "Sri Lanka", "West Indies")) %>%
  filter( team2 %in% c("Pakistan","England","India","New Zealand", "South Africa", "Australia", "Sri Lanka", "West Indies")) %>%
  select(.,-season) %>%
  select(c(1:11))

filenames <- paste0(here("static", "data", "t20s_male_csv2", corr_games$match_id), ".csv")

corr_data <- list()
corr_data <- 
  lapply(filenames,read_csv,show_col_types=FALSE) %>%
  lapply(.,filter, innings==1) %>%
  lapply(., 
         mutate,
         total = cumsum(runs_off_bat + extras),
         wickets=ifelse(is.na(player_dismissed),0,1),
         balls = as.integer(ball) * 6 + (ball - as.integer(ball)) * 10,
         ) %>%
  #lapply(.,mutate, wkt_balls = ifelse(wickets==0,0,(120-balls) * (wickets+1))) %>%
  lapply(.,mutate, wickets = cumsum(wickets)) %>%
  lapply(.,filter, between(ball,5,6) | ball == max(ball)) %>%
  lapply(slice, (n()-1):n()) %>%
  lapply(select,-season) %>%
  bind_rows()

x <- corr_data %>%
  filter(ball < 6)
x$wickets = 10-x$wickets

y <- corr_data %>%
  filter(ball > 6)

ds <- tibble(id= y$match_id, final=y$total, PP_score = x$total, PP_wkt=x$wickets, team=y$batting_team)

#ds$scaled_PP_wkt <- (ds$PP_wkt- min(ds$PP_wkt))/(max(ds$PP_wkt)-min(ds$PP_wkt))
#ds$scaled_PP_score <- (ds$PP_score- min(ds$PP_score))/(max(ds$PP_score)-min(ds$PP_score))

ds$t <- ds$PP_score * ds$PP_wkt 
```

# Analysis
Now that we have the data all set up, we can start the analysis. The first order of business is figuring out the correlation between the independent variables. This is pretty simple in `R`, all we need is the `cor.test()` function which is part of the `stats` pacakge loaded by default in `R` when it starts.

## Correlation between variables
``` {r correlation, include= TRUE}
cor.test(ds$PP_wkt, ds$PP_score)
```
As we can see, the correlation between the variables that are supposed to be independent is quite high. We have two options here:

1. Drop one of the variables, or
2. Combine them into a new variable

We'll take the second option as we do not want to rid ourselves of any predictive power of one of the variables. To combine the two variables, we'll simply multiply them and create a new variable called `t`. This new variable works well enough for our purpose, because it  allows us to have both variables present in our regression. But it also means that we can't separate the impact of each element. At this point, we're happy with the trade-off but this can be revisited later.

### Sidenote on scaling
There are other ways to form a new variable as well. For example, we could also use just their sum, or the sum of their squares (think of the two elements as vectors (x,y) coordinates on a Cartesian plane) in our regression 

However, when we do that, we have to be wary of them having different scales. In our case, the PP scores are in tens while wickets are in ones, so one variable will have a larger impact on the sum or sum of squares. To reduce this, we'll have to scale the variables (using normalization or min-max scaling). When we multiply the variables, we avoid the scaling issue.^[Those stats gave worse forecasting results when I tried them as an experiment, but I will not include the results for brevity. The code, however, does include calculation of scaled stats.]

## Building the Regression
Now that we have dealt with the issue of correlated variables, we can turn our attention to the type of regression that'll be best for our purposes. Let's first graph the data to get a visual idea of what we're dealing with.
``` {r graph_1}
ds %>% ggplot(aes(x = t, y = final)) +
  geom_point() +
  geom_smooth(method="lm", formula= y~x) + 
  ggtitle("Final scores and 't'") +
  labs(caption = "t is the product of Powerplay total and wickets remaining") +
  ggthemes::theme_economist() 
```
As we can clearly see, there is a linear trend present. However, we should also note the variance present alongside the trend. If we take any value of `t` and visualize the difference between the min and max value of the final score on the y-axis, the difference is quite large. So, we should expect our linear model to not do very well in terms of having a high R<sup>2</sup> value. This makes sense too, because the Powerplay is just 6 overs out of 20 in an innings and even with field restrictions it can't be expected to fully dictate the outcome of an innings. 

### Types of Regression
We'll try some non-linear, non-parametric methods as well to fit our data and see if they can do better than a simple linear model. Namely, we'll use loess regression, GAM regression, KNN means regression and SVM regression which are extensively used algorithms in the ML world (more for classification purposes, but also used as regressions).

### Splitting Data
In order to test the forecasting accuracy of our models, we'll have to split our data into two parts. The first part will be used to estimate the data, and the second part will be used to test the accuracy of the forecasts against actual values. We'll arbitrarily use a split of 70-30, as that's the first one that popped in my mind :)

After splitting the data we'll build models of the types discussed above and measure them against each other according to their forecast accuracy. Since we'll be doing the same thing after the models are created, we'll again use the function building approach so that we don't have to write the same code every time.

### Resampling
We'll repeat this whole exercise (i.e. splitting the data, estimating the model and then testing) a 100 times as well. This is so that any bias is removed because a once-done sample could be more favorable to a particular method. Our total sample size is not too large so resampling 100 times should be enough.

```{r models, echo=TRUE, cache=TRUE}
#install caret, e1071, mgcv packages before running chunk

# function to test prediction accuracy
pred_acc <- function(model, data) {
    preds <- predict(model, newdata = data) %>% unname()
    errors <- data$final - preds
    return(errors)
}

all_median <- list()
all_mean <- list()
for (i in 1:100) {
  # splitting data
  set.seed(i) # set seed for reproducability
  train <- slice_sample(ds, prop=0.7)
  test <- anti_join(ds, train, by = "id")

  # creating models
  model_lm <- lm(final ~ t, data = train)
  model_loess <- loess(final ~ t, data = train, control = loess.control(surface = "direct"))
  model_knn <- caret::knnreg(final ~ t, data = train)
  model_svm <- e1071::svm(final ~ t,
                          data = train, kernel = "radial", epsilon = .1)
  model_gam <- mgcv::gam(final ~ s(t, bs = "ts"),
                         data = train, method="REML")

  all_mods <- lst(model_lm, model_loess, model_knn, model_svm, model_gam)

 
  sing_errors <- lapply(all_mods, pred_acc, test) %>% bind_cols()

  # get table
  all_median[[i]] <- (sing_errors %>%
    mutate_all(abs) %>%
    summarize_all(median))
  all_mean[[i]] <- sing_errors %>%
    mutate_all(abs) %>%
    summarize_all(mean)
}
```

## Comparison of Models
```{r tables_errors, echo=FALSE}
table_err <- all_median %>%
  bind_rows() %>%
  summarize_all(mean)

table_err %>%
  gt %>% 
  tab_header(title="Average of Median Absolute Errors") %>% 
  tab_options(column_labels.background.color = "#F9F9F9", column_labels.text_transform = "uppercase") %>%
  fmt_number(columns=everything(),decimals=1) %>%
  opt_row_striping(row_striping = TRUE) %>%
  cols_label(.list = structure(as.list(str_remove(colnames(table_err), ".*_")),
                               names = colnames(table_err)
                               )
             )

table_err <-all_mean %>%
  bind_rows() %>%
  summarize_all(mean)

table_err %>%
  gt %>% 
  tab_header(title="Average of Mean Absolute Errors") %>% 
  tab_options(column_labels.background.color = "#F9F9F9", column_labels.text_transform = "uppercase") %>%
  fmt_number(columns=everything(),decimals=1) %>%
  opt_row_striping(row_striping = TRUE) %>%
  cols_label(.list = structure(as.list(str_remove(colnames(table_err), ".*_")),
                               names = colnames(table_err)
                               )
             )
```
### Comparison of Forecasting Accuracy
As we can see from the tables above, in terms of predictive accuracy, all models are pretty similar, including the linear model (which is quite basic). The tables show us that if we just know the Powerplay score, we can predict the final score within 18 runs or less, half the time and, on average, our prediction will be off by about 22 runs. The visual representation of data had given us the clue that due to high variation, we can't expect to get very close to a final prediction with just the PP score. 

### Comparison of R<sup>2</sup>
We can go on and build a more sophisticated model by incorporating factors like quality of batters to come, quality of bowling etc. but we do not need to. Our purpose was to measure the predictive power of just the Powerplay score and we've done that. We'll now calculate R<sup>2</sup> statistic for all models to measure the goodness of fit to see how much variation in the data is explained by our models.

```{r R_squared, cache=TRUE}
calc_R_sq <- function(model, data){
  preds<- predict(model, newdata=data)
  actuals <- data$final
  SSR <- (preds-actuals)^2 %>% sum
  SST <- (actuals - mean(actuals))^2 %>% sum
  R_sq <- 1-SSR/SST
  return(R_sq)
}
model_lm <- lm(final ~ t, data = ds)
model_loess <- loess(final ~ t, data = ds, control = loess.control(surface = "direct"))
model_knn <- caret::knnreg(final ~ t, data = ds)
model_svm <- e1071::svm(final ~ t,
                          data = ds, kernel = "radial", epsilon = .1)
model_gam <- mgcv::gam(final ~ s(t, bs = "ts"),
                         data = ds, method="REML")

all_mods <- lst(model_lm, model_loess, model_knn, model_svm, model_gam)

sapply(all_mods,calc_R_sq, ds) %>% 
  enframe(name="Model", value="R squared") %>%
  mutate(Model=toupper(str_remove(Model,".*_"))) %>%
  gt() %>%
  tab_options(column_labels.background.color = "#F9F9F9") %>%
  tab_header(title="R squared values") %>% 
  fmt_percent(columns=2, decimals = 1) %>%
  opt_row_striping(row_striping = TRUE)
```
We can see that all models perform similarly except KNN. It is no coincidence that KNN has the highest R<sup>2</sup> and the lowest forecasting accuracy. KNN is overfitting the model and in doing so, compromising on forecast accuracy.

# Model Selection

Based on the above information, we'll use the simple linear model as our measure of Powerplay score because it is pretty much the same as other models and gives us easy interpretation of results (unlike non-parametric methods). As a model validity check, we'll confirm that the errors from the model our independently distributed through the Shapiro-Wilk test for normality.

``` {r histogram, echo= FALSE }
tibble( errors = residuals(model_lm)) %>%
  ggplot(aes(x=errors)) + 
  geom_histogram(stat="bin",binwidth=5, color="black",fill="gray")+
  geom_density(aes(y=after_stat(density)*(length(residuals(model_lm)) * 5)),alpha=0.6, fill="gray") + 
  ylab("Count") +
  ggthemes::theme_economist()
shapiro.test(residuals(model_lm))
```
The null hypothesis of normality is not rejected at 5% significance level, i.e. p-value > 5%, so our model is considered valid